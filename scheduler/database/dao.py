import time
import logging
from typing import List, Dict
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.dialects.postgresql import insert
from sqlalchemy import func
from scheduler.database.models import MarketData, MarketCap, Candle
from datetime import datetime

logger = logging.getLogger(__name__)
BATCH_SIZE = 1000


async def upsert_market_data(db: AsyncSession, data: List[Dict]):
    """
    –ú–∞—Å—Å–æ–≤—ã–π upsert —Å –∑–∞–º–µ—Ä–æ–º –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è.
    –ü–æ–ª—è —Å–æ –∑–Ω–∞—á–µ–Ω–∏–µ–º None –ù–ï –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞—é—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –ë–î ‚Äî
    —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è —Å—Ç–∞—Ä–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (–∑–∞—â–∏—Ç–∞ –æ—Ç –∑–∞—Ç–∏—Ä–∞–Ω–∏—è –¥–∞–Ω–Ω—ã–º–∏ NULL).
    """
    if not data:
        logger.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è upsert ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º.")
        return

    total = len(data)
    logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º upsert {total} –∑–∞–ø–∏—Å–µ–π (–±–∞—Ç—á –ø–æ {BATCH_SIZE})...")

    start_time = time.time()

    try:
        for i in range(0, total, BATCH_SIZE):
            batch_start = time.time()
            batch = data[i:i + BATCH_SIZE]

            stmt = insert(MarketData.__table__).values(batch)
            excluded = stmt.excluded
            table = MarketData.__table__

            # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ù–ï NULL
            update_dict = {}
            for col in batch[0].keys():
                if col in ('secid', 'boardid'):
                    continue
                # COALESCE(new_value, old_value): –µ—Å–ª–∏ new IS NULL ‚Üí –æ—Å—Ç–∞–≤–∏—Ç—å old
                update_dict[col] = func.coalesce(excluded[col], table.c[col])

            stmt = stmt.on_conflict_do_update(
                index_elements=['secid', 'boardid'],
                set_=update_dict
            )

            await db.execute(stmt)
            await db.flush()

            batch_duration = time.time() - batch_start
            logger.debug(f"–ë–∞—Ç—á {i // BATCH_SIZE + 1}: {len(batch)} –∑–∞–ø–∏—Å–µ–π ‚Üí {batch_duration:.3f} —Å–µ–∫")

        await db.commit()

        total_duration = time.time() - start_time
        logger.info(f"–£—Å–ø–µ—à–Ω–æ upserted {total} –∑–∞–ø–∏—Å–µ–π –∑–∞ {total_duration:.3f} —Å–µ–∫")

    except Exception as e:
        await db.rollback()
        total_duration = time.time() - start_time
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ upsert: {e} (–≤—Ä–µ–º—è –¥–æ –æ—à–∏–±–∫–∏: {total_duration:.3f} —Å–µ–∫)", exc_info=True)
        raise


async def upsert_market_cap_data(db: AsyncSession, data: List[Dict]):
    """
    Upsert 1‚Äì2 –∑–∞–ø–∏—Å–µ–π —Ä—ã–Ω–æ—á–Ω–æ–π –∫–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏–∏.
    –û–∂–∏–¥–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤–∏–¥–∞:
    [
        {"timestamp": "2025-09-29", "cap": 51761895103146.734},
        {"timestamp": "2025-09-30 15:16:00", "cap": 51897272299589.94}
    ]
    –í—Å–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –ø—Ä–∏–≤–æ–¥—è—Ç—Å—è –∫ –¥–∞—Ç–µ (YYYY-MM-DD).
    """
    if not data:
        logger.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ –∫–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏–∏ ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º upsert.")
        return

    start_time = time.time()

    try:
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º timestamp ‚Üí date
        normalized = []
        for item in data:
            ts_str = item["timestamp"]
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –¥–∞—Ç—É
            date_str = ts_str.split(" ")[0]  # —Ä–∞–±–æ—Ç–∞–µ—Ç –∏ –¥–ª—è "2025-09-29", –∏ –¥–ª—è "2025-09-30 15:16:00"
            dt = datetime.strptime(date_str, "%Y-%m-%d").date()
            normalized.append({"timestamp": dt, "cap": item["cap"]})

        # –í—ã–ø–æ–ª–Ω—è–µ–º upsert
        stmt = insert(MarketCap).values(normalized)
        stmt = stmt.on_conflict_do_update(
            index_elements=["timestamp"],
            set_={"cap": stmt.excluded.cap}
        )
        await db.execute(stmt)
        await db.commit()

        duration = time.time() - start_time
        logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ upserted {len(normalized)} –∑–∞–ø–∏—Å–µ–π –∫–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–∞ {duration:.3f} —Å–µ–∫")

    except Exception as e:
        await db.rollback()
        duration = time.time() - start_time
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ upsert –∫–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏–∏: {e} (–≤—Ä–µ–º—è: {duration:.3f} —Å–µ–∫)", exc_info=True)
        raise


async def insert_daily_candles(db: AsyncSession, candles: List[Dict]) -> None:
    """
    –í—Å—Ç–∞–≤–ª—è–µ—Ç –¥–Ω–µ–≤–Ω—ã–µ —Å–≤–µ—á–∏. –ò–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ (ticker, date).

    –û–∂–∏–¥–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π –≤–∏–¥–∞:
    [
        {"ticker": "ABIO", "date": date(2025, 10, 11), "close": 66.24, "volume": 7470},
        ...
    ]
    """
    if not candles:
        logger.info("üì≠ –ù–µ—Ç —Å–≤–µ—á–µ–π –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º.")
        return

    total = len(candles)
    logger.info(f"üì• –ù–∞—á–∏–Ω–∞–µ–º –≤—Å—Ç–∞–≤–∫—É {total} —Å–≤–µ—á–µ–π (–±–∞—Ç—á –ø–æ {BATCH_SIZE})...")

    start_time = time.time()

    try:
        for i in range(0, total, BATCH_SIZE):
            batch = candles[i:i + BATCH_SIZE]
            batch_start = time.time()

            stmt = insert(Candle).values(batch)
            stmt = stmt.on_conflict_do_nothing(
                index_elements=["ticker", "date"]  # ‚Üê —Å–æ—Å—Ç–∞–≤–Ω–æ–π –ø–µ—Ä–≤–∏—á–Ω—ã–π –∫–ª—é—á
            )
            await db.execute(stmt)
            await db.flush()

            batch_duration = time.time() - batch_start
            logger.debug(f"–ë–∞—Ç—á {i // BATCH_SIZE + 1}: {len(batch)} —Å–≤–µ—á–µ–π ‚Üí {batch_duration:.3f} —Å–µ–∫")

        await db.commit()
        total_duration = time.time() - start_time
        logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –≤—Å—Ç–∞–≤–ª–µ–Ω–æ –¥–æ {total} —Å–≤–µ—á–µ–π –∑–∞ {total_duration:.3f} —Å–µ–∫ (–¥—É–±–ª–∏–∫–∞—Ç—ã –ø—Ä–æ–∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω—ã)")

    except Exception as e:
        await db.rollback()
        total_duration = time.time() - start_time
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—Å—Ç–∞–≤–∫–µ —Å–≤–µ—á–µ–π: {e} (–≤—Ä–µ–º—è –¥–æ –æ—à–∏–±–∫–∏: {total_duration:.3f} —Å–µ–∫)", exc_info=True)
        raise